{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "msg = \"Hello world\"\n",
    "print(msg)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello world\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# Beat tracking example\n",
    "import librosa\n",
    "\n",
    "# Import IPython.display.Audio to play the audio\n",
    "import IPython.display as ipd\n",
    "\n",
    "# 1. Get the file path to an included audio example\n",
    "filename = librosa.example('nutcracker')\n",
    "\n",
    "# 2. Load the audio as a time series, waveform 'y', represented as a one-dimensional NumPy floating point array.\n",
    "#   Store the sampling rate as 'sr' of y, that is, the number of samples per second of audio.\n",
    "#   By default, all audio is mixed to mono and resampled to 22050 Hz at load time. This behavior can be overriden\n",
    "#   by supplying additional arguments to librosa.load\n",
    "y, sr = librosa.load(filename)\n",
    "ipd.Audio(filename)\n",
    "\n",
    "# 3. Run the default beat tracker\n",
    "#   Output is an estimate of the tempo (in beats per minute)\n",
    "#   and an array of frame numbers corresponding to detected beat events\n",
    "#   Frames correspond to short windows of the signal y, each separated by hop_length = 512\n",
    "#   samples. Librosa uses centered frames, so that the kth frame is centered around sample\n",
    "#   k * hop_length\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "print('Estimated tempo: {:.2f} beats per minute'.format(tempo))\n",
    "\n",
    "# 4. Convert the frame indices of beat events into timestamps\n",
    "#   beat_times will be an array of timestamps (in seconds) corresponding to detected beat events\n",
    "beat_times = librosa.frames_to_time(beat_frames, sr=sr)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimated tempo: 107.67 beats per minute\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# Feature extraction sample\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load the example clip\n",
    "y, sr = librosa.load(librosa.ex('nutcracker'))\n",
    "\n",
    "# Set the hop length; at 22050 Hz, 512 samples ~= 23ms\n",
    "hop_length = 512\n",
    "\n",
    "# Using effects module, separate harmonics(tonal) and percussives(transient) into two waveforms.\n",
    "# y is separated into these two time series and each have the same shape and duration as y.\n",
    "# Motivation(s):\n",
    "#   1. Percussive elements tend to be stronger indicators of rhythmic content, can help provide\n",
    "#       stable beat tracking results\n",
    "#   2. Percussive elements can pollute tonal feature representations (such as chroma) by contributing\n",
    "#       energy across all frequency bands, so we'd be better of without them.\n",
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "\n",
    "# Beat track on the percussive signal\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y_percussive, sr=sr)\n",
    "\n",
    "# From feature module, extract the Mel-frequency cepstral coefficients from the raw signal y\n",
    "# Output of this function is the matrix mfcc, which is a numpy.ndarray of shape (n_mfcc, T)\n",
    "# where T denotes the track duration in frames. Note that we use the same hop_length here as\n",
    "# in the beat tracker, so the detected beat_frames values correspond to columns of mfcc\n",
    "mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "\n",
    "# Use feature manipulation, delta, which computes (smoothed) first-order differences among columns\n",
    "# of its input. Result matrix mfcc_delta has the same shape as the input mfcc.\n",
    "mfcc_delta = librosa.feature.delta(mfcc)\n",
    "\n",
    "# Feature manipulation, sync, which aggregates columns of its input between sample indices (e.g. beat frames)\n",
    "# Here, we've vertically stacked the mfcc and mfcc_delta matrices together. The results of this operation is\n",
    "# a matrix beat_mfcc_delta with the same number of rows as its input, but the number of columns depends on beat_frames.\n",
    "# Each column beat_mfcc_delta[:, k] will be the average of input columns between beat_frames[k] and beat_frames[k+1].\n",
    "# beat_frames will be expanded to span the full range [0, T] so that all data is accounted for.\n",
    "beat_mfcc_delta = librosa.util.sync(np.vstack([mfcc, mfcc_delta]), beat_frames)\n",
    "\n",
    "# Compute chroma features from the harmonic signal\n",
    "# Chromagram will be a numpy.ndarray of shape (12, T), and each row corresponds to a pitch class (e.g., C, C#, etc)\n",
    "# Each column of chromagram is normalized by its peak value, though this behavior can be overridden by setting the\n",
    "# norm parameter\n",
    "chromagram = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n",
    "\n",
    "# Syncronize chroma between beat events\n",
    "# We've replaced the default aggregate operation(average, as used above for MFCCs), with the median\n",
    "# In general, any statistical summarization function can be supplied here, including np.max(), np.min(), np.std()\n",
    "beat_chroma = librosa.util.sync(chromagram, beat_frames, aggregate=np.median)\n",
    "\n",
    "# Finally, stack all beat-synchronous features together\n",
    "# Resulting in a feature matrix, beat_features, of shape (12+13+13, #beat intervals)\n",
    "beat_features = np.vstack([beat_chroma, beat_mfcc_delta])\n",
    "\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "beat_mfcc_delta:  [[-5.96049988e+02 -4.71975800e+02 -4.18740723e+02 ... -2.30209213e+02\n  -1.81633041e+02 -4.22515228e+02]\n [ 5.29752111e+00  1.19491829e+02  1.19481026e+02 ...  6.63411026e+01\n   6.66919479e+01  7.45729675e+01]\n [ 1.79374278e+00  5.01075478e+01  1.31605749e+01 ... -6.48235626e+01\n  -6.93393021e+01 -2.06293182e+01]\n ...\n [-6.54889792e-02 -1.46449924e-01  5.33926859e-02 ... -1.69989929e-01\n   6.22177720e-01  1.29084542e-01]\n [-2.32196786e-02 -1.37379631e-01 -1.14387721e-02 ... -2.49835059e-01\n   2.49134541e-01  8.30471702e-03]\n [ 2.86480542e-02 -2.01024517e-01 -5.54223433e-02 ...  7.50200152e-02\n  -6.15619838e-01  7.62829632e-02]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.8.2 (default, Nov  4 2020, 21:23:28) \n[Clang 12.0.0 (clang-1200.0.32.28)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}