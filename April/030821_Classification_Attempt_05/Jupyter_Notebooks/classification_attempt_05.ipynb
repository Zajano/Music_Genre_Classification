{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7e85ec9bf098c5427e45e2f632dcd4eeff803b007e1abd287d600879388709c1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ape_paths import wav_path\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras import utils\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_genres(root_dir, genre):\n",
    "    mel_specs = []\n",
    "    full_labels = []\n",
    "    for file in os.scandir(root_dir):\n",
    "        if file.is_dir() and file.name == genre:\n",
    "            spects, labels = extract_mel_spectrogram(file, file.name)\n",
    "            # Adding the mel spectrogram to the list\n",
    "            mel_specs += spects\n",
    "            # Extracting the label and adding it to the list\n",
    "            # label = str(file).split('.')[0][11:]\n",
    "            full_labels += labels\n",
    "    return mel_specs, full_labels\n",
    "    # # Converting the list or arrays to an array\n",
    "    # X = np.array(mel_specs)\n",
    "    \n",
    "    # # Converting labels to numeric values\n",
    "    # full_labels = pd.Series(full_labels)\n",
    "    # # print(\"Full labels: \", full_labels, type(full_labels))\n",
    "    # label_dict = {\n",
    "    #     'Ambient Electronic': 0,\n",
    "    #     'Chiptune': 1,\n",
    "    #     'Classical': 2,\n",
    "    #     'Country': 3,\n",
    "    #     'Electronic': 4,\n",
    "    #     'Folk': 5,\n",
    "    #     'Hip-Hop': 6,\n",
    "    #     'Indie-Rock': 7,\n",
    "    #     'Jazz': 8,\n",
    "    #     'Metal': 9,\n",
    "    #     'Pop': 10,\n",
    "    #     'Post-Rock': 11,\n",
    "    #     'Psych-Rock': 12,\n",
    "    #     'Punk': 13,\n",
    "    #     'Reggae': 14,\n",
    "    #     'Rock': 15,\n",
    "    #     'Techno': 16,\n",
    "    #     'Trip-Hop': 17\n",
    "    # }\n",
    "    # y = full_labels.map(label_dict).values\n",
    "    \n",
    "    # # Returning the mel spectrograms and labels\n",
    "    # return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(genre_dir, label):\n",
    "    '''\n",
    "    This function takes in a directory of audio files in .wav format, computes the\n",
    "    mel spectrogram for each audio file, reshapes them so that they are all the \n",
    "    same size, and stores them in a numpy array. \n",
    "    \n",
    "    It also creates a list of genre labels and maps them to numeric values.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (int): a directory of audio files in .wav format\n",
    "    \n",
    "    Returns:\n",
    "    X (array): array of mel spectrogram data from all audio files in the given\n",
    "    directory\n",
    "    y (array): array of the corresponding genre labels in numeric form\n",
    "    '''\n",
    "    \n",
    "    # Creating empty lists for mel spectrograms and labels\n",
    "    labels = []\n",
    "    mel_specs = []\n",
    "    \n",
    "    \n",
    "    # Looping through each file in the directory\n",
    "    for file in os.scandir(genre_dir):\n",
    "        # Don't process if not .mp3 file\n",
    "        if file.name.endswith('.wav'):  \n",
    "            # Loading in the audio file\n",
    "            y, sr = librosa.core.load(file)\n",
    "            \n",
    "            # Extracting the label and adding it to the list\n",
    "            # label = str(file).split('.')[0][11:]\n",
    "            labels.append(label)\n",
    "\n",
    "            # Computing the mel spectrograms\n",
    "            spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n",
    "            spect = librosa.power_to_db(spect, ref=np.max)\n",
    "            \n",
    "            # Adjusting the size to be 128 x 660\n",
    "            if spect.shape[1] != 660:\n",
    "                spect.resize(128,660, refcheck=False)\n",
    "                \n",
    "            # Adding the mel spectrogram to the list\n",
    "            mel_specs.append(spect)\n",
    "    return mel_specs, labels\n",
    "        \n",
    "    # Converting the list or arrays to an array\n",
    "    # X = np.array(mel_specs)\n",
    "    \n",
    "    # # Converting labels to numeric values\n",
    "    # labels = pd.Series(labels)\n",
    "    # label_dict = {\n",
    "    #     'Ambient Electronic': 0,\n",
    "    #     'Chiptune': 1,\n",
    "    #     'Classical': 2,\n",
    "    #     'Country': 3,\n",
    "    #     'Electronic': 4,\n",
    "    #     'Folk': 5,\n",
    "    #     'Hip-Hop': 6,\n",
    "    #     'Indie-Rock': 7,\n",
    "    #     'Jazz': 8,\n",
    "    #     'Metal': 9,\n",
    "    #     'Pop': 10,\n",
    "    #     'Post-Rock': 11,\n",
    "    #     'Psych-Rock': 12,\n",
    "    #     'Punk': 13,\n",
    "    #     'Reggae': 14,\n",
    "    #     'Rock': 15,\n",
    "    #     'Techno': 16,\n",
    "    #     'Trip-Hop': 17\n",
    "    # }\n",
    "    # y = labels.map(label_dict).values\n",
    "    \n",
    "    # # Returning the mel spectrograms and labels\n",
    "    # return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "0",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d69a6649c43e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     }\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Using the function to read and extract mel spectrograms from the GTZAN Genre Dataset audio files\n",
    "genre_dict = {\n",
    "        'Ambient Electronic': 0,\n",
    "        'Chiptune': 1,\n",
    "        'Classical': 2,\n",
    "        'Country': 3,\n",
    "        'Electronic': 4,\n",
    "        'Folk': 5,\n",
    "        'Hip-Hop': 6,\n",
    "        'Indie-Rock': 7,\n",
    "        'Jazz': 8,\n",
    "        'Metal': 9,\n",
    "        'Pop': 10,\n",
    "        'Post-Rock': 11,\n",
    "        'Psych-Rock': 12,\n",
    "        'Punk': 13,\n",
    "        'Reggae': 14,\n",
    "        'Rock': 15,\n",
    "        'Techno': 16,\n",
    "        'Trip-Hop': 17\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, y0 = extract_from_genres(wav_path, 'Ambient Electronic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = extract_from_genres(wav_path, 'Chiptune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = extract_from_genres(wav_path, 'Classical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3, y3 = extract_from_genres(wav_path, 'Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4, y4 = extract_from_genres(wav_path, 'Electronic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5, y5 = extract_from_genres(wav_path, 'Folk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6, y6 = extract_from_genres(wav_path, 'Hip-Hop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X7, y7 = extract_from_genres(wav_path, 'Indie-Rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X8, y8 = extract_from_genres(wav_path, 'Jazz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X9, y9 = extract_from_genres(wav_path, 'Metal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X10, y10 = extract_from_genres(wav_path, 'Pop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X11, y11 = extract_from_genres(wav_path, 'Post-Rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X12, y12 = extract_from_genres(wav_path, 'Psych-Rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X13, y13 = extract_from_genres(wav_path, 'Punk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X14, y14 = extract_from_genres(wav_path, 'Reggae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X15, y15 = extract_from_genres(wav_path, 'Rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X16, y16 = extract_from_genres(wav_path, 'Techno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X17, y17 = extract_from_genres(wav_path, 'Trip-Hop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X0 + X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15 + X16 + X17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y0 + y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12 + y13 + y14 + y15 + y16 + y17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18 470 128\n"
     ]
    }
   ],
   "source": [
    "print(len(X), len(X[2]), len(X[2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-60-dd2302ae54e1>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  X = np.array(X)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping filename column\n",
    "data_no_filenames = data_raw.drop(['filename'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_filenames.shape\n",
    "#data_no_filenames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three data sets, data_no_filenames, data_mfccs_only, and data_feat_mfccs\n",
    "# CLASSIFICATION first features + MFCCs 1-13 only\n",
    "# Remove mfcss over 13, retain genre column\n",
    "mfcc = 'mfcc'\n",
    "mfcc_list = [mfcc+str(x) for x in list(range(13+1 ,20+1))]\n",
    "data_mfccs_plus = data_no_filenames.drop(mfcc_list, axis=1)\n",
    "data_mfccs_plus.shape\n",
    "print(data_mfccs_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the genres into numbers\n",
    "genres_list = data_mfccs_plus.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "genres_y = encoder.fit_transform(genres_list)\n",
    "genres_y.shape\n",
    "# print(genres_y)\n",
    "# Just checking to see the original genres\n",
    "#print(encoder.inverse_transform(genres_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mfccs only\n",
    "data_mfccs_only = data_mfccs_plus.iloc[:,data_mfccs_plus.columns.get_loc('mfcc1'):]\n",
    "data_mfccs_only.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first six features and mfcc2 only\n",
    "mfcc2_drop = [mfcc+str(x) for x in list(range(2+1 ,13+1))]\n",
    "data_feat_mfcc2 = data_mfccs_plus.drop(mfcc2_drop,axis=1)\n",
    "data_feat_mfcc2 = data_feat_mfcc2.drop('mfcc1',axis=1)\n",
    "data_feat_mfcc2.shape\n",
    "#print(data_feat_mfcc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the Feature Columns \n",
    "# a. All\n",
    "scaler = StandardScaler()\n",
    "features_X_a = scaler.fit_transform(np.array(data_no_filenames.iloc[:, :-1], dtype=float))\n",
    "features_X_a.shape\n",
    "#print(features_X_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. MFCCS 1-13 only\n",
    "scaler = StandardScaler()\n",
    "features_X_b = scaler.fit_transform(np.array(data_mfccs_only.iloc[:, :-1], dtype=float))\n",
    "features_X_b.shape\n",
    "print(features_X_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. First six features and mfcc2 only\n",
    "scaler = StandardScaler()\n",
    "features_X_c = scaler.fit_transform(np.array(data_feat_mfcc2.iloc[:, :-1], dtype=float))\n",
    "features_X_c.shape\n",
    "#print(features_X_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFICATION ALL FEATURES\n",
    "# Dividing data into training and Testing set\n",
    "# Test a\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(features_X_a, genres_y, test_size=0.2)\n",
    "# Test b\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(features_X_b, genres_y, test_size=0.2)\n",
    "# Test c\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(features_X_c, genres_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train_a), len(X_train_b), len(X_train_c))\n",
    "print(len(X_test_a), len(X_test_b), len(X_test_c))\n",
    "print(len(y_train_a), len(y_train_b), len(y_train_c))\n",
    "print(len(y_test_a), len(y_test_b), len(y_test_c))\n",
    "print(len(genres_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification with Keras\n",
    "# Relu - Applies the rectified linear unit activation function.\n",
    "# With default values, this returns the standard ReLU activation: max(x, 0), the element-wise maximum of 0 and the input tensor.\n",
    "# https://keras.io/api/layers/activations/\n",
    "# Softmax - The softmax function, also known as softargmax[1]:184 or normalized exponential function,[2]:198 is a generalization of the logistic \n",
    "# function to multiple dimensions. It is used in multinomial logistic regression and is often used as the last activation function of a neural \n",
    "# network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\n",
    "# https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "# Building our Networks\n",
    "# A\n",
    "modelA = models.Sequential()\n",
    "modelA.add(layers.Dense(512, activation='relu', input_shape=(X_train_a.shape[1],)))\n",
    "modelA.add(layers.Dense(256, activation='relu'))\n",
    "modelA.add(layers.Dense(128, activation='relu'))\n",
    "modelA.add(layers.Dense(64, activation='relu'))\n",
    "modelA.add(layers.Dense(20, activation='softmax'))\n",
    "\n",
    "# B\n",
    "modelB = models.Sequential()\n",
    "modelB.add(layers.Dense(512, activation='relu', input_shape=(X_train_b.shape[1],)))\n",
    "modelB.add(layers.Dense(256, activation='relu'))\n",
    "modelB.add(layers.Dense(128, activation='relu'))\n",
    "modelB.add(layers.Dense(64, activation='relu'))\n",
    "modelB.add(layers.Dense(20, activation='softmax'))\n",
    "\n",
    "# C\n",
    "modelC = models.Sequential()\n",
    "modelC.add(layers.Dense(512, activation='relu', input_shape=(X_train_c.shape[1],)))\n",
    "modelC.add(layers.Dense(256, activation='relu'))\n",
    "modelC.add(layers.Dense(128, activation='relu'))\n",
    "modelC.add(layers.Dense(64, activation='relu'))\n",
    "modelC.add(layers.Dense(20, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config the model with losses and metrics\n",
    "# Optimizer that implements the Adam algorithm - https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "# SparseCategoricalCrossentropy computes the crossentropy loss between the labels and predictions. Use this function\n",
    "#   when there are two or more label classes. We expect labels to be provided as integers.\n",
    "# Accuracy calculates how often predictions equal labels.  \n",
    "modelA.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelB.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelC.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set epochs\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# x_train = Input data, numpy array\n",
    "# y_train = Target data, numpy array consistent with x\n",
    "# epochs = integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided.\n",
    "#           The model is not trained for a number of iterations given by epochs, but merely until the epoch of index\n",
    "#           epochs is reached.\n",
    "# batch_size = Integer or None. Number of samples per gradient update. \n",
    "historyA = modelA.fit(X_train_a, y_train_a, epochs=epochs, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyB = modelB.fit(X_train_b, y_train_b, epochs=epochs, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyC = modelC.fit(X_train_c, y_train_c, epochs=epochs, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the loss value and metrics values for the model in test mode. Computation is done in batches.\n",
    "# X_test = Input data\n",
    "# y_test = Target data\n",
    "# batch_size = Number of samples per batch of computation. If unspecified, will default to 32.\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "testA = modelA.evaluate(X_test_a, y_test_a, verbose=0)\n",
    "testB = modelB.evaluate(X_test_b, y_test_b, verbose=0)\n",
    "testC = modelC.evaluate(X_test_c, y_test_c, verbose=0)\n",
    " \n",
    "print('Test loss A: ', testA[0])\n",
    "print('Test loss B: ', testB[0])\n",
    "print('Test loss C: ', testC[0])\n",
    "\n",
    "print('Test accuracy A: ', testA[1])\n",
    "print('Test accuracy B: ', testB[1])\n",
    "print('Test accuracy C: ', testC[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating approach\n",
    "# Set apart 200 samples in training data to use as validation set\n",
    "x_val_a = X_train_a[:200]\n",
    "x_val_b = X_train_b[:200]\n",
    "x_val_c = X_train_c[:200]\n",
    "partial_x_train_a = X_train_a[200:]\n",
    "partial_x_train_b = X_train_b[200:]\n",
    "partial_x_train_c = X_train_c[200:]\n",
    "\n",
    "y_val_a = y_train_a[:200]\n",
    "y_val_b = y_train_b[:200]\n",
    "y_val_c = y_train_c[:200]\n",
    "partial_y_train_a = y_train_a[200:]\n",
    "partial_y_train_b = y_train_b[200:]\n",
    "partial_y_train_c = y_train_c[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train network for epochs = 20\n",
    "# Building our Networks\n",
    "# A\n",
    "modelA = models.Sequential()\n",
    "modelA.add(layers.Dense(512, activation='relu', input_shape=(X_train_a.shape[1],)))\n",
    "modelA.add(layers.Dense(256, activation='relu'))\n",
    "modelA.add(layers.Dense(128, activation='relu'))\n",
    "modelA.add(layers.Dense(64, activation='relu'))\n",
    "modelA.add(layers.Dense(20, activation='softmax'))\n",
    "\n",
    "# B\n",
    "modelB = models.Sequential()\n",
    "modelB.add(layers.Dense(512, activation='relu', input_shape=(X_train_b.shape[1],)))\n",
    "modelB.add(layers.Dense(256, activation='relu'))\n",
    "modelB.add(layers.Dense(128, activation='relu'))\n",
    "modelB.add(layers.Dense(64, activation='relu'))\n",
    "modelB.add(layers.Dense(20, activation='softmax'))\n",
    "\n",
    "# C\n",
    "modelC = models.Sequential()\n",
    "modelC.add(layers.Dense(512, activation='relu', input_shape=(X_train_c.shape[1],)))\n",
    "modelC.add(layers.Dense(256, activation='relu'))\n",
    "modelC.add(layers.Dense(128, activation='relu'))\n",
    "modelC.add(layers.Dense(64, activation='relu'))\n",
    "modelC.add(layers.Dense(20, activation='softmax'))\n",
    "\n",
    "modelA.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelB.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "modelC.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyA = modelA.fit(partial_x_train_a, partial_y_train_a, epochs=epochs, batch_size=512, validation_data=(x_val_a, y_val_a))\n",
    "resultsA = modelA.evaluate(X_test_a, y_test_a)\n",
    "print(resultsA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyB = modelB.fit(partial_x_train_b, partial_y_train_b, epochs=epochs, batch_size=512, validation_data=(x_val_b, y_val_b))\n",
    "resultsB = modelB.evaluate(X_test_b, y_test_b)\n",
    "print(resultsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyC = modelC.fit(partial_x_train_c, partial_y_train_c, epochs=epochs, batch_size=512, validation_data=(x_val_c, y_val_c))\n",
    "resultsC = modelC.evaluate(X_test_c, y_test_c)\n",
    "print(resultsC)"
   ]
  }
 ]
}