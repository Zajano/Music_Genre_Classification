{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# from ape_paths import wav_path\n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras import utils\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "z_path = \"C:\\\\Users\\\\Zack\\\\Desktop\\\\work\\\\OSU\\\\467_capstone\\\\data\\\\audio_samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_genres(root_dir, genre):\n",
    "    mel_specs = []\n",
    "    full_labels = []\n",
    "    for file in os.scandir(root_dir):\n",
    "        if file.is_dir() and file.name == genre:\n",
    "            print(file.name)\n",
    "            print(type(file.name))\n",
    "            spects, labels = extract_mel_spectrogram(file, file.name)\n",
    "            # Adding the mel spectrogram to the list\n",
    "            mel_specs += spects\n",
    "            # Extracting the label and adding it to the list\n",
    "            # label = str(file).split('.')[0][11:]\n",
    "            full_labels += labels\n",
    "    return mel_specs, full_labels\n",
    "    # # Converting the list or arrays to an array\n",
    "    # X = np.array(mel_specs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(genre_dir, label):\n",
    "    '''\n",
    "    This function takes in a directory of audio files in .wav format, computes the\n",
    "    mel spectrogram for each audio file, reshapes them so that they are all the \n",
    "    same size, and stores them in a numpy array. \n",
    "    \n",
    "    It also creates a list of genre labels and maps them to numeric values.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (int): a directory of audio files in .wav format\n",
    "    \n",
    "    Returns:\n",
    "    X (array): array of mel spectrogram data from all audio files in the given\n",
    "    directory\n",
    "    y (array): array of the corresponding genre labels in numeric form\n",
    "    '''\n",
    "    \n",
    "    # Creating empty lists for mel spectrograms and labels\n",
    "    labels = []\n",
    "    mel_specs = []\n",
    "    \n",
    "    \n",
    "    # Looping through each file in the directory\n",
    "    for file in os.scandir(genre_dir):\n",
    "        # Don't process if not .mp3 file\n",
    "        # if file.name.endswith('.wav'):  \n",
    "        # Loading in the audio file\n",
    "        print(file)\n",
    "        y, sr = librosa.core.load(file)\n",
    "        \n",
    "        # Extracting the label and adding it to the list\n",
    "        # label = str(file).split('.')[0][11:]\n",
    "        labels.append(label)\n",
    "\n",
    "        # Computing the mel spectrograms\n",
    "        spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n",
    "        spect = librosa.power_to_db(spect, ref=np.max)\n",
    "        \n",
    "        # Adjusting the size to be 128 x 660\n",
    "        if spect.shape[1] != 660:\n",
    "            spect.resize(128,660, refcheck=False)\n",
    "            \n",
    "        # Adding the mel spectrogram to the list\n",
    "        mel_specs.append(spect)\n",
    "    return mel_specs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function to read and extract mel spectrograms from the GTZAN Genre Dataset audio files\n",
    "genre_dict = {\n",
    "        'Ambient Electronic': 0,\n",
    "        'Chiptune': 1,\n",
    "        'Classical': 2,\n",
    "        'Country': 3,\n",
    "        'Electronic': 4,\n",
    "        'Folk': 5,\n",
    "        'Hip-Hop': 6,\n",
    "        'Indie-Rock': 7,\n",
    "        'Jazz': 8,\n",
    "        'Metal': 9,\n",
    "        'Pop': 10,\n",
    "        'Post-Rock': 11,\n",
    "        'Psych-Rock': 12,\n",
    "        'Punk': 13,\n",
    "        'Reggae': 14,\n",
    "        'Rock': 15,\n",
    "        'Techno': 16,\n",
    "        'Trip-Hop': 17\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Error opening 'C:\\\\Users\\\\Zack\\\\Desktop\\\\work\\\\OSU\\\\467_capstone\\\\data\\\\audio_samples\\\\Ambient Electronic\\\\001550.mp3': File contains data in an unknown format.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-225ef17a9584>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_from_genres\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Ambient Electronic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-950d3eeeb0c5>\u001b[0m in \u001b[0;36mextract_from_genres\u001b[1;34m(root_dir, genre)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgenre\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mspects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_mel_spectrogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[1;31m# Adding the mel spectrogram to the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mmel_specs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mspects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-2d1f4536358a>\u001b[0m in \u001b[0;36mextract_mel_spectrogram\u001b[1;34m(genre_dir, label)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# if file.name.endswith('.wav'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Loading in the audio file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Extracting the label and adding it to the list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr_native\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__audioread_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;31m# Final cleanup for dtype and contiguity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoundFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[0msr_native\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    627\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[1;32m--> 629\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'r+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[1;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid file: {0!r}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m         _error_check(_snd.sf_error(file_ptr),\n\u001b[0m\u001b[0;32m   1184\u001b[0m                      \"Error opening {0!r}: \".format(self.name))\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[1;34m(err, prefix)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1357\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error opening 'C:\\\\Users\\\\Zack\\\\Desktop\\\\work\\\\OSU\\\\467_capstone\\\\data\\\\audio_samples\\\\Ambient Electronic\\\\001550.mp3': File contains data in an unknown format."
     ]
    }
   ],
   "source": [
    "X0, y0 = extract_from_genres(z_path, 'Ambient Electronic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = extract_from_genres(z_path, 'Chiptune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = extract_from_genres(z_path, 'Classical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3, y3 = extract_from_genres(z_path, 'Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4, y4 = extract_from_genres(z_path, 'Electronic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5, y5 = extract_from_genres(z_path, 'Folk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6, y6 = extract_from_genres(z_path, 'Hip-Hop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X7, y7 = extract_from_genres(z_path, 'Indie-Rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X8, y8 = extract_from_genres(z_path, 'Jazz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X9, y9 = extract_from_genres(z_path, 'Metal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X10, y10 = extract_from_genres(z_path, 'Pop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X11, y11 = extract_from_genres(z_path, 'Post-Rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X12, y12 = extract_from_genres(z_path, 'Psych-Rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X13, y13 = extract_from_genres(z_path, 'Punk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X14, y14 = extract_from_genres(z_path, 'Reggae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X15, y15 = extract_from_genres(z_path, 'Rock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X16, y16 = extract_from_genres(z_path, 'Techno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X17, y17 = extract_from_genres(z_path, 'Trip-Hop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X0 + X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 + X14 + X15 + X16 + X17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y0 + y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 + y12 + y13 + y14 + y15 + y16 + y17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-bf73b6d2a9b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(len(X), len(X[2]), len(X[2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting labels to numeric values\n",
    "full_labels = pd.Series(y)\n",
    "# print(\"Full labels: \", full_labels, type(full_labels))\n",
    "label_dict = {\n",
    "    'Ambient Electronic': 0,\n",
    "    'Chiptune': 1,\n",
    "    'Classical': 2,\n",
    "    'Country': 3,\n",
    "    'Electronic': 4,\n",
    "    'Folk': 5,\n",
    "    'Hip-Hop': 6,\n",
    "    'Indie-Rock': 7,\n",
    "    'Jazz': 8,\n",
    "    'Metal': 9,\n",
    "    'Pop': 10,\n",
    "    'Post-Rock': 11,\n",
    "    'Psych-Rock': 12,\n",
    "    'Punk': 13,\n",
    "    'Reggae': 14,\n",
    "    'Rock': 15,\n",
    "    'Techno': 16,\n",
    "    'Trip-Hop': 17\n",
    "}\n",
    "y = full_labels.map(label_dict).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play this\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the minimum value (the scale ranges from zero to some negative value) to see how we should scale the data\n",
    "min_val = X_train.min()\n",
    "print(min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling our data to be between 0 and 1 using the minimum value from above\n",
    "X_train /= min_val\n",
    "X_test /= min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping images to be 128 x 660 x 1, where the 1 represents the single color channel\n",
    "X_train = X_train.reshape(X_train.shape[0], 128, 660, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 128, 660, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding our labels\n",
    "y_train = to_categorical(y_train, num_classes=18)\n",
    "y_test = to_categorical(y_test, num_classes=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "# Initializing a random seed for replication purposes\n",
    "np.random.seed(12345)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# Initiating an empty neural network\n",
    "cnn_model = Sequential(name='cnn_1')\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model.add(Conv2D(filters=16,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu',\n",
    "                     input_shape=(128,660,1)))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn_model.add(Conv2D(filters=32,\n",
    "                     kernel_size=(3,3),\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding max pooling layer\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,4)))\n",
    "\n",
    "# Adding a flattened layer to input our image data\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# Adding a dense layer with 64 neurons\n",
    "cnn_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Adding a dropout layer for regularization\n",
    "cnn_model.add(Dropout(0.25))\n",
    "\n",
    "# Adding an output layer\n",
    "cnn_model.add(Dense(18, activation='softmax'))\n",
    "\n",
    "# Compiling our neural network\n",
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Fitting our neural network\n",
    "history = cnn_model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size=16,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the model summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was adapted from a lecture at General Assembly\n",
    "\n",
    "# Check out our train loss and test loss over epochs.\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "plt.plot(test_loss, label='Testing Loss', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Loss by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Categorical Crossentropy', fontsize = 18)\n",
    "plt.xticks(range(1,16), range(1,16))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell was adapted from a lecture at General Assembly\n",
    "\n",
    "# Check out our train accuracy and test accuracy over epochs.\n",
    "train_loss = history.history['accuracy']\n",
    "test_loss = history.history['val_accuracy']\n",
    "\n",
    "# Set figure size.\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate line plot of training, testing loss over epochs.\n",
    "plt.plot(train_loss, label='Training Accuracy', color='blue')\n",
    "plt.plot(test_loss, label='Testing Accuracy', color='red')\n",
    "\n",
    "# Set title\n",
    "plt.title('Training and Testing Accuracy by Epoch', fontsize = 25)\n",
    "plt.xlabel('Epoch', fontsize = 18)\n",
    "plt.ylabel('Accuracy', fontsize = 18)\n",
    "plt.xticks(range(1,21), range(1,21))\n",
    "\n",
    "plt.legend(fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss: ', test[0])\n",
    "print('Test accuracy: ', test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions from the cnn model\n",
    "predictions = cnn_model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONFUSION MATRIX\n",
    "# Checking the number of targets per class\n",
    "for i in range(18): \n",
    "    print(f'{i}: {sum([1 for target in y_test if target[i] == 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of predicted values in each class\n",
    "for i in range(18): \n",
    "    print(f'{i}: {sum([1 for prediction in predictions if np.argmax(prediction) == i])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix \n",
    "# row: actual\n",
    "# columns: predicted\n",
    "conf_matrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(predictions, 1))\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of the confusion matrix with labels for readability \n",
    "confusion_df = pd.DataFrame(conf_matrix)\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of a subset of the genres\n",
    "labels_dict = {\n",
    "    0: 'Ambient Electronic',\n",
    "    1: 'Chiptune',\n",
    "    2: 'Classical',\n",
    "    3: 'Country',\n",
    "    4: 'Electronic',\n",
    "    5: 'Folk',\n",
    "    6: 'Hip-Hop',\n",
    "    7: 'Indie-Rock',\n",
    "    8: 'Jazz',\n",
    "    9: 'Metal',\n",
    "    10: 'Pop',\n",
    "    11: 'Post-Rock',\n",
    "    12: 'Psych-Rock',\n",
    "    13: 'Punk',\n",
    "    14: 'Reggae',\n",
    "    15: 'Rock',\n",
    "    16: 'Techno',\n",
    "    17: 'Trip-Hop'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming rows and columns with labes\n",
    "confusion_df = confusion_df.rename(columns=labels_dict)\n",
    "confusion_df.index = confusion_df.columns\n",
    "confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a heatmap for the confusion matrix for display\n",
    "plt.figure(figsize= (20,12))\n",
    "sns.set(font_scale = 2);\n",
    "ax = sns.heatmap(confusion_df, annot=True, cmap=sns.cubehelix_palette(50));\n",
    "ax.set(xlabel='Predicted Values', ylabel='Actual Values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "\n",
    "# # swtich to model directory\n",
    "# os.chdir(r\"/Volumes/APE_External/Dropbox/_Classes/21_Winter/CS_467/Project_Folder/Music_Genre_Classification/Model\")\n",
    "\n",
    "# # create and save file\n",
    "# joblib_file = \"model_030921.joblib\"\n",
    "# dump(cnn_model, joblib_file)"
   ]
  },
  {
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# cnn_model.save('/Volumes/APE_External/Dropbox/_Classes/21_Winter/CS_467/Project_Folder/Music_Genre_Classification/Model/model_030921.h5')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\Zack\\\\Desktop\\\\work\\\\OSU\\\\467_capstone\\\\Genre_classification\\\\Model')\n",
    "model = load_model('model_030921.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}